{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0249feb5",
   "metadata": {},
   "source": [
    "### This is the notebook of answers for work in pandas on strata scratch medium questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f31f66dc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Highest Cost orders - Shopify\n",
    "Find the customer with the highest daily total order cost between 2019-02-01 to 2019-05-01. If customer had more than one order on a certain day, sum the order costs on daily basis. Output customer's first name, total cost of their items, and the date.\n",
    "\n",
    "\n",
    "For simplicity, you can assume that every first name in the dataset is unique.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create the dataframe - combining the customer and order info\n",
    "cust_orders = orders[['cust_id','order_date','total_order_cost']].merge(customers[['id','first_name']], how = 'left', left_on = 'cust_id', right_on = 'id')[['cust_id','first_name','order_date','total_order_cost']]\n",
    "# daily_totals\n",
    "cust_orders['daily_totals'] = cust_orders.groupby(['cust_id','order_date'])['total_order_cost'].transform('sum')\n",
    "# now drop duplicates, order the file etc\n",
    "cust_orders = cust_orders.copy()[['first_name','order_date','daily_totals']].drop_duplicates()\n",
    "# rank the daily_sales col\n",
    "cust_orders['rank'] = cust_orders['daily_totals'].rank(ascending = False, method = 'dense')\n",
    "# bam! \n",
    "cust_orders[cust_orders['rank']==1][['first_name','order_date','daily_totals']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21b9eb10",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "New Products - Tesla\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10318\n",
    "\n",
    "\n",
    "Calculate the net change in the number of products launched by companies in 2020 compared to 2019. Your output should include the company names and the net difference.\n",
    "(Net difference = Number of products launched in 2020 - The number launched in 2019.)\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get yearly brand counts\n",
    "car_launches['year_'] = car_launches.groupby(['year','company_name']).transform('count')\n",
    "# 2019\n",
    "car19 = car_launches[car_launches['year']==2019][['company_name','year_']]\n",
    "# 2020\n",
    "car20 = car_launches[car_launches['year']==2020][['company_name','year_']]\n",
    "# full dataset\n",
    "cars = car19.merge(car20, how = 'left', on = 'company_name',suffixes=('19', '20')).fillna(0)\n",
    "# yoy change\n",
    "cars['diff'] = cars['year_20'] - cars['year_19']\n",
    "# boom\n",
    "cars[['company_name', 'diff']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68a40e13",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Users By Average Session Time - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10352\n",
    "\n",
    "Calculate each user's average session time, where a session is defined as the time difference between a page_load and a page_exit. Assume each user has only one session per day. If there are multiple page_load or page_exit events on the same day, use only the latest page_load and the earliest page_exit, ensuring the page_load occurs before the page_exit. Output the user_id and their average session time.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# get page load events at user level\n",
    "users_start = facebook_web_log[facebook_web_log.action == 'page_load'][['user_id','timestamp']]\n",
    "# get the actual date\n",
    "users_start['date'] = users_start['timestamp'].dt.date\n",
    "# rank the logins by date - to latest login at date level\n",
    "users_start['rank'] = users_start.groupby(['user_id', 'date'])['timestamp'].rank(ascending=False, method = 'first')\n",
    "# now limit it to only the rank of 1\n",
    "user_start = users_start[users_start['rank']==1].copy()\n",
    "# get page exit events at user level\n",
    "users_end = facebook_web_log[facebook_web_log.action == 'page_exit'][['user_id','timestamp']]\n",
    "# get the actual date\n",
    "users_end['date'] = users_end['timestamp'].dt.date\n",
    "# rank the exits by date - to earliest exit at date level\n",
    "users_end['rank'] = users_end.groupby(['user_id','date'])['timestamp'].rank(method = 'first')\n",
    "# limit it to just rank of 1\n",
    "user_end = users_end[users_end['rank']==1].copy()\n",
    "# join the dataframes - inner basis - dont want nulls here!\n",
    "user_info = user_start.merge(user_end, how='inner', on = ['user_id','date'])[['user_id','timestamp_x','timestamp_y']].rename(columns={'timestamp_x':'start','timestamp_y':'end'})\n",
    "# create the timedelta differences\n",
    "user_info['sec'] = user_info.apply(lambda row: row['end'] - row['start'], axis = 1)\n",
    "# bam the answer - agg returns only the one row per person!\n",
    "user_info[['user_id','sec']].groupby('user_id')['sec'].agg('mean').reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cb962ba",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Acceptance Rate By Date - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10285\n",
    "\n",
    "Calculate the friend acceptance rate for each date when friend requests were sent. A request is sent if action = sent and accepted if action = accepted. If a request is not accepted, there is no record of it being accepted in the table. The output will only include dates where requests were sent and at least one of them was accepted, as the acceptance rate can only be calculated for those dates. Show the results ordered from the earliest to the latest date.\n",
    "\n",
    "## my answer \n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get the sent\n",
    "fb_sends = fb_friend_requests[fb_friend_requests['action']=='sent'][['date','user_id_sender', 'user_id_receiver']]\n",
    "# get the accepted ones\n",
    "fb_received = fb_friend_requests[fb_friend_requests['action']=='accepted'][['user_id_sender', 'user_id_receiver', 'action']]\n",
    "# merge the sets\n",
    "fb_accepted = fb_sends.merge(fb_received, how = 'left', left_on = ['user_id_sender', 'user_id_receiver'], right_on = ['user_id_sender','user_id_receiver'])\n",
    "# variable to indicate sent\n",
    "fb_accepted['sent'] = 1\n",
    "fb_accepted['daily_sent']=fb_accepted.groupby('date')['sent'].transform('sum')\n",
    "# var to indicate accepte\n",
    "fb_accepted['accepted'] = fb_accepted['action'].transform(lambda x : 1 if x == 'accepted' else 0)\n",
    "fb_accepted['daily_accepted']=fb_accepted.groupby('date')['accepted'].transform('sum')\n",
    "# daily acceptance_rate\n",
    "fb_accepted['daily_acceptance'] = fb_accepted['daily_accepted']/fb_accepted['daily_sent']\n",
    "# boom!\n",
    "fb_accepted[['date','daily_acceptance']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "301ad7b5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Finding User Purchases - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10322\n",
    "\n",
    "Identify returning active users by finding users who made a second purchase within 1 to 7 days after their first purchase. Ignore same-day purchases. Output a list of these user_ids.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# filter the file and get a ranking variable\n",
    "first = amazon_transactions.copy()[['user_id','created_at']].sort_values(['user_id','created_at'])\n",
    "first['rank'] = first.groupby('user_id')['created_at'].rank(method = 'first')\n",
    "# first visit for everyone\n",
    "one_vis = first[first['rank']==1][['user_id','created_at']]\n",
    "# second visit for everyone - if it exists\n",
    "two_vis = first[first['rank']==2][['user_id','created_at']]\n",
    "# merge the two visits together into a df\n",
    "both_visits = one_vis.merge(two_vis, how = 'left', on = 'user_id').rename(columns={'created_at_x':'first','created_at_y':'second'}).dropna()\n",
    "# calc days between first and second visit\n",
    "both_visits['days_between'] = (both_visits['second'] - both_visits['first']).dt.days\n",
    "# calculate our diffs and filter\n",
    "both_visits[both_visits['days_between']<=7]['user_id']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a04684a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Premium vs Freemium - Microsoft\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10300\n",
    "\n",
    "Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads. Hint: In Oracle you should use \"date\" when referring to date column (reserved keyword).\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "user_acct = ms_user_dimension.merge(ms_acc_dimension, how = 'left', right_on ='acc_id', left_on='acc_id')\n",
    "#get if user is a paying customer\n",
    "user_acct['paying'] = user_acct['paying_customer'].transform(lambda x : 1 if x.lower() == 'yes' else 0)\n",
    "# remove unnessecary cols\n",
    "user_acct = user_acct.drop('paying_customer', axis = 1).copy()[['user_id','paying']]\n",
    "# get non paying flag\n",
    "user_acct['non_paying'] = user_acct['paying'].transform(lambda x : 1 if x == 0 else 0)\n",
    "#get to final data set\n",
    "user_acct_final = ms_download_facts.merge(user_acct, how = 'left')[['date','downloads','paying','non_paying']].copy()\n",
    "#create the overall pay/nonpay downloads vars\n",
    "user_acct_final[['pay','non_pay']] = user_acct_final.apply(lambda row : (row['paying']*row['downloads'],row['non_paying']*row['downloads']), axis = 1,result_type = 'expand')\n",
    "#final cols we need\n",
    "user_acct_final = user_acct_final[['date', 'non_pay','pay']].copy()\n",
    "# creates final dataset and sort\n",
    "user_final = user_acct_final.groupby('date').agg('sum').reset_index().sort_values('date')\n",
    "# filter and sort\n",
    "user_final[user_final['non_pay']>user_final['pay']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57b39c47",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Risky Projects - LinkedIn\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10304\n",
    "\n",
    "You are given a set of projects and employee data. Each project has a name, a budget, and a specific duration, while each employee has an annual salary and may be assigned to one or more projects for particular periods. The task is to identify which projects are overbudget. A project is considered overbudget if the prorated cost of all employees assigned to it exceeds the project’s budget.\n",
    "To solve this, you must prorate each employee's annual salary based on the exact period they work on a given project, relative to a full year. For example, if an employee works on a six-month project, only half of their annual salary should be attributed to that project. Sum these prorated salary amounts for all employees assigned to a project and compare the total with the project’s budget.\n",
    "Your output should be a list of overbudget projects, where each entry includes the project’s name, its budget, and the total prorated employee expenses for that project. The total expenses should be rounded up to the nearest dollar. Assume all years have 365 days and disregard leap years.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "# merge all the dfs together - get the needed fields\n",
    "li = linkedin_employees.merge(linkedin_emp_projects, how = 'left', left_on = 'id', right_on = 'emp_id')[['emp_id','project_id','salary']].merge(linkedin_projects[['id', 'title','budget','start_date','end_date']], left_on = 'project_id', right_on = 'id', how = 'left').drop_duplicates()\n",
    "# prorate the salary and calc total cost\n",
    "li['sal'] = (((li['end_date']-li['start_date']).dt.days/365)*li['salary'])\n",
    "# this groups by project title, sums the budget, get the ceiling\n",
    "li['proj_budget'] = li.groupby('title')['sal'].transform(lambda x : math.ceil(sum(x)))\n",
    "# boom answer\n",
    "li[li['proj_budget']>li['budget']][['title','budget','proj_budget']].sort_values('title').drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6cf39de",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Activity Rank - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10351\n",
    "\n",
    "Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. The user with the highest number of emails sent will have a rank of 1, and so on. Output the user, total emails, and their activity rank.\n",
    "\n",
    "\n",
    "•\tOrder records first by the total emails in descending order.\n",
    "•\tThen, sort users with the same number of emails in alphabetical order by their username.\n",
    "•\tIn your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "# the transform is way more efficient for this! it allows for the row counts!\n",
    "google_gmail_emails['emails_sent'] = google_gmail_emails.groupby('from_user')['from_user'].transform('count')\n",
    "# this gives us just a singular value per user\n",
    "google_gmail_emails = google_gmail_emails[['from_user','emails_sent']].drop_duplicates().copy()\n",
    "# sort the values as desired\n",
    "google_gmail_emails = google_gmail_emails.sort_values(['emails_sent', 'from_user'], ascending = [False, True]).copy()\n",
    "# now we can use the first method to get our row number ranking\n",
    "google_gmail_emails['rank'] = google_gmail_emails['emails_sent'].rank(method='first', ascending = False).copy()\n",
    "# got it!\n",
    "google_gmail_emails"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46178409",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Customer Revenue In March - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9782\n",
    "\n",
    "Calculate the total revenue from each customer in March 2019. Include only customers who were active in March 2019. An active user is a customer who made at least one transaction in March 2019.\n",
    "Output the revenue along with the customer id and sort the results based on the revenue in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# filter for march\n",
    "orders = orders[(orders['order_date'].dt.month == 3)&(orders['order_date'].dt.year == 2019)]\n",
    "# get cust totals info - use transform for grouped info\n",
    "orders['cust_tot'] = orders.groupby('cust_id')['total_order_cost'].transform('sum')\n",
    "# sort and present!\n",
    "orders[['cust_id','cust_tot']].drop_duplicates().sort_values('cust_tot', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95181b3f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Count Occurrences Of Words In Drafts - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9817\n",
    "\n",
    "Find the number of times each word appears in the contents column across all rows in the google_file_store dataset. Output two columns: word and occurrences.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# string split these \n",
    "words = google_file_store['contents'].str.split(' ')\n",
    "# expand the list of words fully\n",
    "word = words.explode('contents').reset_index()\n",
    "# remove non textual content\n",
    "word['contents'] = word.contents.str.replace('[^a-zA-Z]', '').str.lower()\n",
    "word['counts'] = word.groupby('contents').transform('count')\n",
    "word = word.copy()[['contents','counts']].sort_values('counts',ascending=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed943ba2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
