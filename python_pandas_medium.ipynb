{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0249feb5",
   "metadata": {},
   "source": [
    "### This is the notebook of answers for work in pandas on strata scratch medium questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f31f66dc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Highest Cost orders - Shopify\n",
    "Find the customer with the highest daily total order cost between 2019-02-01 to 2019-05-01. If customer had more than one order on a certain day, sum the order costs on daily basis. Output customer's first name, total cost of their items, and the date.\n",
    "\n",
    "\n",
    "For simplicity, you can assume that every first name in the dataset is unique.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create the dataframe - combining the customer and order info\n",
    "cust_orders = orders[['cust_id','order_date','total_order_cost']].merge(customers[['id','first_name']], how = 'left', left_on = 'cust_id', right_on = 'id')[['cust_id','first_name','order_date','total_order_cost']]\n",
    "# daily_totals\n",
    "cust_orders['daily_totals'] = cust_orders.groupby(['cust_id','order_date'])['total_order_cost'].transform('sum')\n",
    "# now drop duplicates, order the file etc\n",
    "cust_orders = cust_orders.copy()[['first_name','order_date','daily_totals']].drop_duplicates()\n",
    "# rank the daily_sales col\n",
    "cust_orders['rank'] = cust_orders['daily_totals'].rank(ascending = False, method = 'dense')\n",
    "# bam! \n",
    "cust_orders[cust_orders['rank']==1][['first_name','order_date','daily_totals']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21b9eb10",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "New Products - Tesla\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10318\n",
    "\n",
    "\n",
    "Calculate the net change in the number of products launched by companies in 2020 compared to 2019. Your output should include the company names and the net difference.\n",
    "(Net difference = Number of products launched in 2020 - The number launched in 2019.)\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get yearly brand counts\n",
    "car_launches['year_'] = car_launches.groupby(['year','company_name']).transform('count')\n",
    "# 2019\n",
    "car19 = car_launches[car_launches['year']==2019][['company_name','year_']]\n",
    "# 2020\n",
    "car20 = car_launches[car_launches['year']==2020][['company_name','year_']]\n",
    "# full dataset\n",
    "cars = car19.merge(car20, how = 'left', on = 'company_name',suffixes=('19', '20')).fillna(0)\n",
    "# yoy change\n",
    "cars['diff'] = cars['year_20'] - cars['year_19']\n",
    "# boom\n",
    "cars[['company_name', 'diff']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68a40e13",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Users By Average Session Time - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10352\n",
    "\n",
    "Calculate each user's average session time, where a session is defined as the time difference between a page_load and a page_exit. Assume each user has only one session per day. If there are multiple page_load or page_exit events on the same day, use only the latest page_load and the earliest page_exit, ensuring the page_load occurs before the page_exit. Output the user_id and their average session time.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# get page load events at user level\n",
    "users_start = facebook_web_log[facebook_web_log.action == 'page_load'][['user_id','timestamp']]\n",
    "# get the actual date\n",
    "users_start['date'] = users_start['timestamp'].dt.date\n",
    "# rank the logins by date - to latest login at date level\n",
    "users_start['rank'] = users_start.groupby(['user_id', 'date'])['timestamp'].rank(ascending=False, method = 'first')\n",
    "# now limit it to only the rank of 1\n",
    "user_start = users_start[users_start['rank']==1].copy()\n",
    "# get page exit events at user level\n",
    "users_end = facebook_web_log[facebook_web_log.action == 'page_exit'][['user_id','timestamp']]\n",
    "# get the actual date\n",
    "users_end['date'] = users_end['timestamp'].dt.date\n",
    "# rank the exits by date - to earliest exit at date level\n",
    "users_end['rank'] = users_end.groupby(['user_id','date'])['timestamp'].rank(method = 'first')\n",
    "# limit it to just rank of 1\n",
    "user_end = users_end[users_end['rank']==1].copy()\n",
    "# join the dataframes - inner basis - dont want nulls here!\n",
    "user_info = user_start.merge(user_end, how='inner', on = ['user_id','date'])[['user_id','timestamp_x','timestamp_y']].rename(columns={'timestamp_x':'start','timestamp_y':'end'})\n",
    "# create the timedelta differences\n",
    "user_info['sec'] = user_info.apply(lambda row: row['end'] - row['start'], axis = 1)\n",
    "# bam the answer - agg returns only the one row per person!\n",
    "user_info[['user_id','sec']].groupby('user_id')['sec'].agg('mean').reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cb962ba",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Acceptance Rate By Date - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10285\n",
    "\n",
    "Calculate the friend acceptance rate for each date when friend requests were sent. A request is sent if action = sent and accepted if action = accepted. If a request is not accepted, there is no record of it being accepted in the table. The output will only include dates where requests were sent and at least one of them was accepted, as the acceptance rate can only be calculated for those dates. Show the results ordered from the earliest to the latest date.\n",
    "\n",
    "## my answer \n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get the sent\n",
    "fb_sends = fb_friend_requests[fb_friend_requests['action']=='sent'][['date','user_id_sender', 'user_id_receiver']]\n",
    "# get the accepted ones\n",
    "fb_received = fb_friend_requests[fb_friend_requests['action']=='accepted'][['user_id_sender', 'user_id_receiver', 'action']]\n",
    "# merge the sets\n",
    "fb_accepted = fb_sends.merge(fb_received, how = 'left', left_on = ['user_id_sender', 'user_id_receiver'], right_on = ['user_id_sender','user_id_receiver'])\n",
    "# variable to indicate sent\n",
    "fb_accepted['sent'] = 1\n",
    "fb_accepted['daily_sent']=fb_accepted.groupby('date')['sent'].transform('sum')\n",
    "# var to indicate accepte\n",
    "fb_accepted['accepted'] = fb_accepted['action'].transform(lambda x : 1 if x == 'accepted' else 0)\n",
    "fb_accepted['daily_accepted']=fb_accepted.groupby('date')['accepted'].transform('sum')\n",
    "# daily acceptance_rate\n",
    "fb_accepted['daily_acceptance'] = fb_accepted['daily_accepted']/fb_accepted['daily_sent']\n",
    "# boom!\n",
    "fb_accepted[['date','daily_acceptance']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "301ad7b5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Finding User Purchases - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10322\n",
    "\n",
    "Identify returning active users by finding users who made a second purchase within 1 to 7 days after their first purchase. Ignore same-day purchases. Output a list of these user_ids.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# filter the file and get a ranking variable\n",
    "first = amazon_transactions.copy()[['user_id','created_at']].sort_values(['user_id','created_at'])\n",
    "first['rank'] = first.groupby('user_id')['created_at'].rank(method = 'first')\n",
    "# first visit for everyone\n",
    "one_vis = first[first['rank']==1][['user_id','created_at']]\n",
    "# second visit for everyone - if it exists\n",
    "two_vis = first[first['rank']==2][['user_id','created_at']]\n",
    "# merge the two visits together into a df\n",
    "both_visits = one_vis.merge(two_vis, how = 'left', on = 'user_id').rename(columns={'created_at_x':'first','created_at_y':'second'}).dropna()\n",
    "# calc days between first and second visit\n",
    "both_visits['days_between'] = (both_visits['second'] - both_visits['first']).dt.days\n",
    "# calculate our diffs and filter\n",
    "both_visits[both_visits['days_between']<=7]['user_id']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a04684a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Premium vs Freemium - Microsoft\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10300\n",
    "\n",
    "Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads. Hint: In Oracle you should use \"date\" when referring to date column (reserved keyword).\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "user_acct = ms_user_dimension.merge(ms_acc_dimension, how = 'left', right_on ='acc_id', left_on='acc_id')\n",
    "#get if user is a paying customer\n",
    "user_acct['paying'] = user_acct['paying_customer'].transform(lambda x : 1 if x.lower() == 'yes' else 0)\n",
    "# remove unnessecary cols\n",
    "user_acct = user_acct.drop('paying_customer', axis = 1).copy()[['user_id','paying']]\n",
    "# get non paying flag\n",
    "user_acct['non_paying'] = user_acct['paying'].transform(lambda x : 1 if x == 0 else 0)\n",
    "#get to final data set\n",
    "user_acct_final = ms_download_facts.merge(user_acct, how = 'left')[['date','downloads','paying','non_paying']].copy()\n",
    "#create the overall pay/nonpay downloads vars\n",
    "user_acct_final[['pay','non_pay']] = user_acct_final.apply(lambda row : (row['paying']*row['downloads'],row['non_paying']*row['downloads']), axis = 1,result_type = 'expand')\n",
    "#final cols we need\n",
    "user_acct_final = user_acct_final[['date', 'non_pay','pay']].copy()\n",
    "# creates final dataset and sort\n",
    "user_final = user_acct_final.groupby('date').agg('sum').reset_index().sort_values('date')\n",
    "# filter and sort\n",
    "user_final[user_final['non_pay']>user_final['pay']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57b39c47",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Risky Projects - LinkedIn\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10304\n",
    "\n",
    "You are given a set of projects and employee data. Each project has a name, a budget, and a specific duration, while each employee has an annual salary and may be assigned to one or more projects for particular periods. The task is to identify which projects are overbudget. A project is considered overbudget if the prorated cost of all employees assigned to it exceeds the project’s budget.\n",
    "To solve this, you must prorate each employee's annual salary based on the exact period they work on a given project, relative to a full year. For example, if an employee works on a six-month project, only half of their annual salary should be attributed to that project. Sum these prorated salary amounts for all employees assigned to a project and compare the total with the project’s budget.\n",
    "Your output should be a list of overbudget projects, where each entry includes the project’s name, its budget, and the total prorated employee expenses for that project. The total expenses should be rounded up to the nearest dollar. Assume all years have 365 days and disregard leap years.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "# merge all the dfs together - get the needed fields\n",
    "li = linkedin_employees.merge(linkedin_emp_projects, how = 'left', left_on = 'id', right_on = 'emp_id')[['emp_id','project_id','salary']].merge(linkedin_projects[['id', 'title','budget','start_date','end_date']], left_on = 'project_id', right_on = 'id', how = 'left').drop_duplicates()\n",
    "# prorate the salary and calc total cost\n",
    "li['sal'] = (((li['end_date']-li['start_date']).dt.days/365)*li['salary'])\n",
    "# this groups by project title, sums the budget, get the ceiling\n",
    "li['proj_budget'] = li.groupby('title')['sal'].transform(lambda x : math.ceil(sum(x)))\n",
    "# boom answer\n",
    "li[li['proj_budget']>li['budget']][['title','budget','proj_budget']].sort_values('title').drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6cf39de",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Activity Rank - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10351\n",
    "\n",
    "Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. The user with the highest number of emails sent will have a rank of 1, and so on. Output the user, total emails, and their activity rank.\n",
    "\n",
    "\n",
    "•\tOrder records first by the total emails in descending order.\n",
    "•\tThen, sort users with the same number of emails in alphabetical order by their username.\n",
    "•\tIn your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "# the transform is way more efficient for this! it allows for the row counts!\n",
    "google_gmail_emails['emails_sent'] = google_gmail_emails.groupby('from_user')['from_user'].transform('count')\n",
    "# this gives us just a singular value per user\n",
    "google_gmail_emails = google_gmail_emails[['from_user','emails_sent']].drop_duplicates().copy()\n",
    "# sort the values as desired\n",
    "google_gmail_emails = google_gmail_emails.sort_values(['emails_sent', 'from_user'], ascending = [False, True]).copy()\n",
    "# now we can use the first method to get our row number ranking\n",
    "google_gmail_emails['rank'] = google_gmail_emails['emails_sent'].rank(method='first', ascending = False).copy()\n",
    "# got it!\n",
    "google_gmail_emails"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46178409",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Customer Revenue In March - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9782\n",
    "\n",
    "Calculate the total revenue from each customer in March 2019. Include only customers who were active in March 2019. An active user is a customer who made at least one transaction in March 2019.\n",
    "Output the revenue along with the customer id and sort the results based on the revenue in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# filter for march\n",
    "orders = orders[(orders['order_date'].dt.month == 3)&(orders['order_date'].dt.year == 2019)]\n",
    "# get cust totals info - use transform for grouped info\n",
    "orders['cust_tot'] = orders.groupby('cust_id')['total_order_cost'].transform('sum')\n",
    "# sort and present!\n",
    "orders[['cust_id','cust_tot']].drop_duplicates().sort_values('cust_tot', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95181b3f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Count Occurrences Of Words In Drafts - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9817\n",
    "\n",
    "Find the number of times each word appears in the contents column across all rows in the google_file_store dataset. Output two columns: word and occurrences.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# string split these \n",
    "words = google_file_store['contents'].str.split(' ')\n",
    "# expand the list of words fully\n",
    "word = words.explode('contents').reset_index()\n",
    "# remove non textual content\n",
    "word['contents'] = word.contents.str.replace('[^a-zA-Z]', '').str.lower()\n",
    "word['counts'] = word.groupby('contents').transform('count')\n",
    "word = word.copy()[['contents','counts']].sort_values('counts',ascending=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed943ba2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Titanic Survivors and Non-Survivors - Tesla\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9881\n",
    "\n",
    "Make a report showing the number of survivors and non-survivors by passenger class. Classes are categorized based on the pclass value as:\n",
    "\n",
    "•\tFirst class: pclass = 1\n",
    "•\tSecond class: pclass = 2\n",
    "•\tThird class: pclass = 3\n",
    "\n",
    "Output the number of survivors and non-survivors by each class.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create the 3 cols we need \n",
    "titanic= titanic.copy()[['passengerid','pclass','survived']]\n",
    "# create the total of survivors/perishers - remember - we want this to be a count\n",
    "titanic['class_survived'] = titanic.groupby(['survived', 'pclass'])['survived'].transform('count')\n",
    "# we only need these 3 cols - drop duplicates\n",
    "titanic = titanic[['pclass','survived','class_survived']].copy().drop_duplicates()\n",
    "# pivot the data\n",
    "titanic_pivot = titanic.pivot(index='survived', columns='pclass', values='class_survived').reset_index()\n",
    "# rename the cols\n",
    "cols = [['survived','first_class','second_class','third_class']]\n",
    "#replace the col names\n",
    "titanic_pivot.columns = cols\n",
    "titanic_pivot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "200067f2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Second Highest Salary - DropBox\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9892\n",
    "\n",
    "Find the second highest salary of employees.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# to rownum over FULL dataframe, make the variable in every row!\n",
    "employee['vals']=1\n",
    "# rank he salaries! cumcount starts at 0 and creates a row number\n",
    "employee['sal_rank'] = employee.sort_values('salary', ascending = False).groupby('vals').cumcount()+1\n",
    "#get the value!\n",
    "employee[employee['sal_rank']==2]['salary']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4566829b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Employee and Manager Salaries\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9894\n",
    "\n",
    "Find employees who are earning more than their managers. Output the employee's first name along with the corresponding salary.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# here we join the cols we want - and only keep the 3 cals we need for our filtering\n",
    "emp_comparison = employee[['first_name','salary','manager_id']].merge(employee[['salary','id']], how = 'left', left_on = 'manager_id', right_on = 'id').rename(columns = {'salary_x':'emp_sal',\"salary_y\":'man_sal'})[['first_name','emp_sal','man_sal']]\n",
    "# filter and done\n",
    "emp_comparison[emp_comparison['emp_sal']>emp_comparison['man_sal']][['first_name','emp_sal']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289a49c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Highest Salary In Department - Asana\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9897\n",
    "\n",
    "Find the employee with the highest salary per department.\n",
    "Output the department name, employee's first name along with the corresponding salary.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get department sal rankings\n",
    "employee['dept_rank'] = employee.groupby('department')['salary'].rank(ascending = False, method = 'first')\n",
    "# filter\n",
    "employee[employee['dept_rank']==1][['department','first_name','salary']].sort_values('department')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7679ec1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Highest Target Under Manager - Saleforce\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9905\n",
    "\n",
    "Identify the employee(s) working under manager manager_id=13 who have achieved the highest target. Return each such employee’s first name alongside the target value. The goal is to display the maximum target among all employees under manager_id=13 and show which employee(s) reached that top value.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create a ranking variable for target gpartitioned by the manager\n",
    "salesforce_employees['rank'] = salesforce_employees.groupby('manager_id')['target'].rank(ascending = False, method = 'dense')\n",
    "# filter the file for mgr 13 and target == 1 and get right cols\n",
    "salesforce_employees[(salesforce_employees['rank']==1)&(salesforce_employees['manager_id']==13)][['first_name','target']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efd1076e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Largest Olympics - ESPN\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9942\n",
    "\n",
    "Find the Olympics with the highest number of unique athletes. The Olympics game is a combination of the year and the season, and is found in the games column. Output the Olympics along with the corresponding number of athletes. The id column uniquely identifies an athlete.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "# get the game name and the unique athelete id\n",
    "olympics = olympics_athletes_events[['games','id']].drop_duplicates()\n",
    "# create a count var for the number of atheletes in each games\n",
    "olympics['atheletes'] = olympics.groupby('games')['id'].transform('count')\n",
    "# only keep the games and atheletes count, drop dupes\n",
    "olympics1 = olympics.copy()[['games','atheletes']].drop_duplicates()\n",
    "# rank in desc order the games by # of atheletes\n",
    "olympics1['rank'] = olympics1['atheletes'].rank(ascending = False)\n",
    "# boom\n",
    "olympics1[olympics1['rank']==1][['games','atheletes']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d044f52",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Top Businesses With Most Reviews - Yelp\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10048\n",
    "\n",
    "Find the top 5 businesses with most reviews. Assume that each row has a unique business_id such that the total reviews for each business is listed on each row. Output the business name along with the total number of reviews and order your results by the total reviews in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get the names and review counts\n",
    "yelp = yelp_business.copy()[['name','review_count']].drop_duplicates()\n",
    "# create the rank variable\n",
    "yelp['rank'] = yelp['review_count'].rank(method = 'dense', ascending = False)\n",
    "# limit to rank <= 5 \n",
    "yelp[yelp['rank']<=5].sort_values('rank')[['name','review_count']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ca68f9f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Reviews of Categories - Yelp\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10049\n",
    "\n",
    "Calculate number of reviews for every business category. Output the category along with the total number of reviews. Order by total reviews in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# only need two columns\n",
    "yelp = yelp_business[['categories','review_count']]\n",
    "# now I can explode - or expand these 2 cols \n",
    "# here I string split\n",
    "yelp['categories'] = yelp['categories'].str.split(';')\n",
    "# here I explode it out - making the df longer\n",
    "yelp = yelp.copy().explode('categories')\n",
    "# now I aggregate\n",
    "yelp['totals'] = yelp.groupby('categories')['review_count'].transform('sum')\n",
    "yelp[['categories','totals']].drop_duplicates().sort_values('totals', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf469ea7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Top Cool Votes - Yelp\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10060\n",
    "\n",
    "Find the review_text that received the highest number of  cool votes.\n",
    "Output the business name along with the review text with the highest number of cool votes.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "yelp_reviews['Rank']=yelp_reviews.cool.rank(method = 'dense', ascending = False)\n",
    "yelp_reviews[yelp_reviews.Rank ==1][['business_name','review_text']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5776e217",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Income By Title and Gender - City of SF\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10077\n",
    "\n",
    "Find the average total compensation based on employee titles and gender. Total compensation is calculated by adding both the salary and bonus of each employee. However, not every employee receives a bonus so disregard employees without bonuses in your calculation. Employee can receive more than one bonus.\n",
    "Output the employee title, gender (i.e., sex), along with the average total compensation.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get sals\n",
    "sf_emp = sf_employee[['id','employee_title','sex','salary']]\n",
    "# this fills 0 for people with null bonus \n",
    "sf_bonus['bonus']= sf_bonus['bonus'].fillna(0)\n",
    "# aggregate the tot bonus\n",
    "sf_bonus['tot_bonus'] = sf_bonus.groupby('worker_ref_id')['bonus'].transform('sum')\n",
    "#remove dupes\n",
    "sf_bonus = sf_bonus[['worker_ref_id','tot_bonus']].drop_duplicates()\n",
    "#merge the sets\n",
    "sf_gender = sf_emp.merge(sf_bonus, left_on ='id', right_on ='worker_ref_id', how = 'left').dropna().copy()[['sex','employee_title','salary','tot_bonus']]\n",
    "#calc averages \n",
    "sf_gender[['salary','tot_bonus']] = sf_gender.groupby(['employee_title','sex'])['salary','tot_bonus'].transform('mean')\n",
    "# drop dupes\n",
    "sf_gender = sf_gender.copy().drop_duplicates()\n",
    "# create the avg variable - remmeber the avg of the avg IS the avg\n",
    "sf_gender['avg_tot_comp']= sf_gender['salary']+sf_gender['tot_bonus']\n",
    "#filter\n",
    "sf_gender[['employee_title','sex','avg_tot_comp']].sort_values('employee_title')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b6481e8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
