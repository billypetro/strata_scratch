{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0249feb5",
   "metadata": {},
   "source": [
    "### This is the notebook of answers for work in pandas on strata scratch medium questions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f31f66dc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Highest Cost orders - Shopify\n",
    "Find the customer with the highest daily total order cost between 2019-02-01 to 2019-05-01. If customer had more than one order on a certain day, sum the order costs on daily basis. Output customer's first name, total cost of their items, and the date.\n",
    "\n",
    "\n",
    "For simplicity, you can assume that every first name in the dataset is unique.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create the dataframe - combining the customer and order info\n",
    "cust_orders = orders[['cust_id','order_date','total_order_cost']].merge(customers[['id','first_name']], how = 'left', left_on = 'cust_id', right_on = 'id')[['cust_id','first_name','order_date','total_order_cost']]\n",
    "# daily_totals\n",
    "cust_orders['daily_totals'] = cust_orders.groupby(['cust_id','order_date'])['total_order_cost'].transform('sum')\n",
    "# now drop duplicates, order the file etc\n",
    "cust_orders = cust_orders.copy()[['first_name','order_date','daily_totals']].drop_duplicates()\n",
    "# rank the daily_sales col\n",
    "cust_orders['rank'] = cust_orders['daily_totals'].rank(ascending = False, method = 'dense')\n",
    "# bam! \n",
    "cust_orders[cust_orders['rank']==1][['first_name','order_date','daily_totals']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21b9eb10",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "New Products - Tesla\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10318\n",
    "\n",
    "\n",
    "Calculate the net change in the number of products launched by companies in 2020 compared to 2019. Your output should include the company names and the net difference.\n",
    "(Net difference = Number of products launched in 2020 - The number launched in 2019.)\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get yearly brand counts\n",
    "car_launches['year_'] = car_launches.groupby(['year','company_name']).transform('count')\n",
    "# 2019\n",
    "car19 = car_launches[car_launches['year']==2019][['company_name','year_']]\n",
    "# 2020\n",
    "car20 = car_launches[car_launches['year']==2020][['company_name','year_']]\n",
    "# full dataset\n",
    "cars = car19.merge(car20, how = 'left', on = 'company_name',suffixes=('19', '20')).fillna(0)\n",
    "# yoy change\n",
    "cars['diff'] = cars['year_20'] - cars['year_19']\n",
    "# boom\n",
    "cars[['company_name', 'diff']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68a40e13",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Users By Average Session Time - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10352\n",
    "\n",
    "Calculate each user's average session time, where a session is defined as the time difference between a page_load and a page_exit. Assume each user has only one session per day. If there are multiple page_load or page_exit events on the same day, use only the latest page_load and the earliest page_exit, ensuring the page_load occurs before the page_exit. Output the user_id and their average session time.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# get page load events at user level\n",
    "users_start = facebook_web_log[facebook_web_log.action == 'page_load'][['user_id','timestamp']]\n",
    "# get the actual date\n",
    "users_start['date'] = users_start['timestamp'].dt.date\n",
    "# rank the logins by date - to latest login at date level\n",
    "users_start['rank'] = users_start.groupby(['user_id', 'date'])['timestamp'].rank(ascending=False, method = 'first')\n",
    "# now limit it to only the rank of 1\n",
    "user_start = users_start[users_start['rank']==1].copy()\n",
    "# get page exit events at user level\n",
    "users_end = facebook_web_log[facebook_web_log.action == 'page_exit'][['user_id','timestamp']]\n",
    "# get the actual date\n",
    "users_end['date'] = users_end['timestamp'].dt.date\n",
    "# rank the exits by date - to earliest exit at date level\n",
    "users_end['rank'] = users_end.groupby(['user_id','date'])['timestamp'].rank(method = 'first')\n",
    "# limit it to just rank of 1\n",
    "user_end = users_end[users_end['rank']==1].copy()\n",
    "# join the dataframes - inner basis - dont want nulls here!\n",
    "user_info = user_start.merge(user_end, how='inner', on = ['user_id','date'])[['user_id','timestamp_x','timestamp_y']].rename(columns={'timestamp_x':'start','timestamp_y':'end'})\n",
    "# create the timedelta differences\n",
    "user_info['sec'] = user_info.apply(lambda row: row['end'] - row['start'], axis = 1)\n",
    "# bam the answer - agg returns only the one row per person!\n",
    "user_info[['user_id','sec']].groupby('user_id')['sec'].agg('mean').reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cb962ba",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Acceptance Rate By Date - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10285\n",
    "\n",
    "Calculate the friend acceptance rate for each date when friend requests were sent. A request is sent if action = sent and accepted if action = accepted. If a request is not accepted, there is no record of it being accepted in the table. The output will only include dates where requests were sent and at least one of them was accepted, as the acceptance rate can only be calculated for those dates. Show the results ordered from the earliest to the latest date.\n",
    "\n",
    "## my answer \n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get the sent\n",
    "fb_sends = fb_friend_requests[fb_friend_requests['action']=='sent'][['date','user_id_sender', 'user_id_receiver']]\n",
    "# get the accepted ones\n",
    "fb_received = fb_friend_requests[fb_friend_requests['action']=='accepted'][['user_id_sender', 'user_id_receiver', 'action']]\n",
    "# merge the sets\n",
    "fb_accepted = fb_sends.merge(fb_received, how = 'left', left_on = ['user_id_sender', 'user_id_receiver'], right_on = ['user_id_sender','user_id_receiver'])\n",
    "# variable to indicate sent\n",
    "fb_accepted['sent'] = 1\n",
    "fb_accepted['daily_sent']=fb_accepted.groupby('date')['sent'].transform('sum')\n",
    "# var to indicate accepte\n",
    "fb_accepted['accepted'] = fb_accepted['action'].transform(lambda x : 1 if x == 'accepted' else 0)\n",
    "fb_accepted['daily_accepted']=fb_accepted.groupby('date')['accepted'].transform('sum')\n",
    "# daily acceptance_rate\n",
    "fb_accepted['daily_acceptance'] = fb_accepted['daily_accepted']/fb_accepted['daily_sent']\n",
    "# boom!\n",
    "fb_accepted[['date','daily_acceptance']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "301ad7b5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Finding User Purchases - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10322\n",
    "\n",
    "Identify returning active users by finding users who made a second purchase within 1 to 7 days after their first purchase. Ignore same-day purchases. Output a list of these user_ids.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# filter the file and get a ranking variable\n",
    "first = amazon_transactions.copy()[['user_id','created_at']].sort_values(['user_id','created_at'])\n",
    "first['rank'] = first.groupby('user_id')['created_at'].rank(method = 'first')\n",
    "# first visit for everyone\n",
    "one_vis = first[first['rank']==1][['user_id','created_at']]\n",
    "# second visit for everyone - if it exists\n",
    "two_vis = first[first['rank']==2][['user_id','created_at']]\n",
    "# merge the two visits together into a df\n",
    "both_visits = one_vis.merge(two_vis, how = 'left', on = 'user_id').rename(columns={'created_at_x':'first','created_at_y':'second'}).dropna()\n",
    "# calc days between first and second visit\n",
    "both_visits['days_between'] = (both_visits['second'] - both_visits['first']).dt.days\n",
    "# calculate our diffs and filter\n",
    "both_visits[both_visits['days_between']<=7]['user_id']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a04684a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Premium vs Freemium - Microsoft\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10300\n",
    "\n",
    "Find the total number of downloads for paying and non-paying users by date. Include only records where non-paying customers have more downloads than paying customers. The output should be sorted by earliest date first and contain 3 columns date, non-paying downloads, paying downloads. Hint: In Oracle you should use \"date\" when referring to date column (reserved keyword).\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "user_acct = ms_user_dimension.merge(ms_acc_dimension, how = 'left', right_on ='acc_id', left_on='acc_id')\n",
    "#get if user is a paying customer\n",
    "user_acct['paying'] = user_acct['paying_customer'].transform(lambda x : 1 if x.lower() == 'yes' else 0)\n",
    "# remove unnessecary cols\n",
    "user_acct = user_acct.drop('paying_customer', axis = 1).copy()[['user_id','paying']]\n",
    "# get non paying flag\n",
    "user_acct['non_paying'] = user_acct['paying'].transform(lambda x : 1 if x == 0 else 0)\n",
    "#get to final data set\n",
    "user_acct_final = ms_download_facts.merge(user_acct, how = 'left')[['date','downloads','paying','non_paying']].copy()\n",
    "#create the overall pay/nonpay downloads vars\n",
    "user_acct_final[['pay','non_pay']] = user_acct_final.apply(lambda row : (row['paying']*row['downloads'],row['non_paying']*row['downloads']), axis = 1,result_type = 'expand')\n",
    "#final cols we need\n",
    "user_acct_final = user_acct_final[['date', 'non_pay','pay']].copy()\n",
    "# creates final dataset and sort\n",
    "user_final = user_acct_final.groupby('date').agg('sum').reset_index().sort_values('date')\n",
    "# filter and sort\n",
    "user_final[user_final['non_pay']>user_final['pay']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57b39c47",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Risky Projects - LinkedIn\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10304\n",
    "\n",
    "You are given a set of projects and employee data. Each project has a name, a budget, and a specific duration, while each employee has an annual salary and may be assigned to one or more projects for particular periods. The task is to identify which projects are overbudget. A project is considered overbudget if the prorated cost of all employees assigned to it exceeds the project’s budget.\n",
    "To solve this, you must prorate each employee's annual salary based on the exact period they work on a given project, relative to a full year. For example, if an employee works on a six-month project, only half of their annual salary should be attributed to that project. Sum these prorated salary amounts for all employees assigned to a project and compare the total with the project’s budget.\n",
    "Your output should be a list of overbudget projects, where each entry includes the project’s name, its budget, and the total prorated employee expenses for that project. The total expenses should be rounded up to the nearest dollar. Assume all years have 365 days and disregard leap years.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "# merge all the dfs together - get the needed fields\n",
    "li = linkedin_employees.merge(linkedin_emp_projects, how = 'left', left_on = 'id', right_on = 'emp_id')[['emp_id','project_id','salary']].merge(linkedin_projects[['id', 'title','budget','start_date','end_date']], left_on = 'project_id', right_on = 'id', how = 'left').drop_duplicates()\n",
    "# prorate the salary and calc total cost\n",
    "li['sal'] = (((li['end_date']-li['start_date']).dt.days/365)*li['salary'])\n",
    "# this groups by project title, sums the budget, get the ceiling\n",
    "li['proj_budget'] = li.groupby('title')['sal'].transform(lambda x : math.ceil(sum(x)))\n",
    "# boom answer\n",
    "li[li['proj_budget']>li['budget']][['title','budget','proj_budget']].sort_values('title').drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6cf39de",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Activity Rank - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10351\n",
    "\n",
    "Find the email activity rank for each user. Email activity rank is defined by the total number of emails sent. The user with the highest number of emails sent will have a rank of 1, and so on. Output the user, total emails, and their activity rank.\n",
    "\n",
    "\n",
    "•\tOrder records first by the total emails in descending order.\n",
    "•\tThen, sort users with the same number of emails in alphabetical order by their username.\n",
    "•\tIn your rankings, return a unique value (i.e., a unique rank) even if multiple users have the same number of emails.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "# the transform is way more efficient for this! it allows for the row counts!\n",
    "google_gmail_emails['emails_sent'] = google_gmail_emails.groupby('from_user')['from_user'].transform('count')\n",
    "# this gives us just a singular value per user\n",
    "google_gmail_emails = google_gmail_emails[['from_user','emails_sent']].drop_duplicates().copy()\n",
    "# sort the values as desired\n",
    "google_gmail_emails = google_gmail_emails.sort_values(['emails_sent', 'from_user'], ascending = [False, True]).copy()\n",
    "# now we can use the first method to get our row number ranking\n",
    "google_gmail_emails['rank'] = google_gmail_emails['emails_sent'].rank(method='first', ascending = False).copy()\n",
    "# got it!\n",
    "google_gmail_emails"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46178409",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Customer Revenue In March - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9782\n",
    "\n",
    "Calculate the total revenue from each customer in March 2019. Include only customers who were active in March 2019. An active user is a customer who made at least one transaction in March 2019.\n",
    "Output the revenue along with the customer id and sort the results based on the revenue in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# filter for march\n",
    "orders = orders[(orders['order_date'].dt.month == 3)&(orders['order_date'].dt.year == 2019)]\n",
    "# get cust totals info - use transform for grouped info\n",
    "orders['cust_tot'] = orders.groupby('cust_id')['total_order_cost'].transform('sum')\n",
    "# sort and present!\n",
    "orders[['cust_id','cust_tot']].drop_duplicates().sort_values('cust_tot', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95181b3f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Count Occurrences Of Words In Drafts - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9817\n",
    "\n",
    "Find the number of times each word appears in the contents column across all rows in the google_file_store dataset. Output two columns: word and occurrences.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# string split these \n",
    "words = google_file_store['contents'].str.split(' ')\n",
    "# expand the list of words fully\n",
    "word = words.explode('contents').reset_index()\n",
    "# remove non textual content\n",
    "word['contents'] = word.contents.str.replace('[^a-zA-Z]', '').str.lower()\n",
    "word['counts'] = word.groupby('contents').transform('count')\n",
    "word = word.copy()[['contents','counts']].sort_values('counts',ascending=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed943ba2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Titanic Survivors and Non-Survivors - Tesla\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9881\n",
    "\n",
    "Make a report showing the number of survivors and non-survivors by passenger class. Classes are categorized based on the pclass value as:\n",
    "\n",
    "•\tFirst class: pclass = 1\n",
    "•\tSecond class: pclass = 2\n",
    "•\tThird class: pclass = 3\n",
    "\n",
    "Output the number of survivors and non-survivors by each class.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create the 3 cols we need \n",
    "titanic= titanic.copy()[['passengerid','pclass','survived']]\n",
    "# create the total of survivors/perishers - remember - we want this to be a count\n",
    "titanic['class_survived'] = titanic.groupby(['survived', 'pclass'])['survived'].transform('count')\n",
    "# we only need these 3 cols - drop duplicates\n",
    "titanic = titanic[['pclass','survived','class_survived']].copy().drop_duplicates()\n",
    "# pivot the data\n",
    "titanic_pivot = titanic.pivot(index='survived', columns='pclass', values='class_survived').reset_index()\n",
    "# rename the cols\n",
    "cols = [['survived','first_class','second_class','third_class']]\n",
    "#replace the col names\n",
    "titanic_pivot.columns = cols\n",
    "titanic_pivot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "200067f2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Second Highest Salary - DropBox\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9892\n",
    "\n",
    "Find the second highest salary of employees.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# to rownum over FULL dataframe, make the variable in every row!\n",
    "employee['vals']=1\n",
    "# rank he salaries! cumcount starts at 0 and creates a row number\n",
    "employee['sal_rank'] = employee.sort_values('salary', ascending = False).groupby('vals').cumcount()+1\n",
    "#get the value!\n",
    "employee[employee['sal_rank']==2]['salary']\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4566829b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Employee and Manager Salaries\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9894\n",
    "\n",
    "Find employees who are earning more than their managers. Output the employee's first name along with the corresponding salary.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# here we join the cols we want - and only keep the 3 cals we need for our filtering\n",
    "emp_comparison = employee[['first_name','salary','manager_id']].merge(employee[['salary','id']], how = 'left', left_on = 'manager_id', right_on = 'id').rename(columns = {'salary_x':'emp_sal',\"salary_y\":'man_sal'})[['first_name','emp_sal','man_sal']]\n",
    "# filter and done\n",
    "emp_comparison[emp_comparison['emp_sal']>emp_comparison['man_sal']][['first_name','emp_sal']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "289a49c1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Highest Salary In Department - Asana\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9897\n",
    "\n",
    "Find the employee with the highest salary per department.\n",
    "Output the department name, employee's first name along with the corresponding salary.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get department sal rankings\n",
    "employee['dept_rank'] = employee.groupby('department')['salary'].rank(ascending = False, method = 'first')\n",
    "# filter\n",
    "employee[employee['dept_rank']==1][['department','first_name','salary']].sort_values('department')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7679ec1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Highest Target Under Manager - Saleforce\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9905\n",
    "\n",
    "Identify the employee(s) working under manager manager_id=13 who have achieved the highest target. Return each such employee’s first name alongside the target value. The goal is to display the maximum target among all employees under manager_id=13 and show which employee(s) reached that top value.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create a ranking variable for target gpartitioned by the manager\n",
    "salesforce_employees['rank'] = salesforce_employees.groupby('manager_id')['target'].rank(ascending = False, method = 'dense')\n",
    "# filter the file for mgr 13 and target == 1 and get right cols\n",
    "salesforce_employees[(salesforce_employees['rank']==1)&(salesforce_employees['manager_id']==13)][['first_name','target']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "efd1076e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Largest Olympics - ESPN\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9942\n",
    "\n",
    "Find the Olympics with the highest number of unique athletes. The Olympics game is a combination of the year and the season, and is found in the games column. Output the Olympics along with the corresponding number of athletes. The id column uniquely identifies an athlete.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "# get the game name and the unique athelete id\n",
    "olympics = olympics_athletes_events[['games','id']].drop_duplicates()\n",
    "# create a count var for the number of atheletes in each games\n",
    "olympics['atheletes'] = olympics.groupby('games')['id'].transform('count')\n",
    "# only keep the games and atheletes count, drop dupes\n",
    "olympics1 = olympics.copy()[['games','atheletes']].drop_duplicates()\n",
    "# rank in desc order the games by # of atheletes\n",
    "olympics1['rank'] = olympics1['atheletes'].rank(ascending = False)\n",
    "# boom\n",
    "olympics1[olympics1['rank']==1][['games','atheletes']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d044f52",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Top Businesses With Most Reviews - Yelp\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10048\n",
    "\n",
    "Find the top 5 businesses with most reviews. Assume that each row has a unique business_id such that the total reviews for each business is listed on each row. Output the business name along with the total number of reviews and order your results by the total reviews in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get the names and review counts\n",
    "yelp = yelp_business.copy()[['name','review_count']].drop_duplicates()\n",
    "# create the rank variable\n",
    "yelp['rank'] = yelp['review_count'].rank(method = 'dense', ascending = False)\n",
    "# limit to rank <= 5 \n",
    "yelp[yelp['rank']<=5].sort_values('rank')[['name','review_count']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ca68f9f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Reviews of Categories - Yelp\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10049\n",
    "\n",
    "Calculate number of reviews for every business category. Output the category along with the total number of reviews. Order by total reviews in descending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# only need two columns\n",
    "yelp = yelp_business[['categories','review_count']]\n",
    "# now I can explode - or expand these 2 cols \n",
    "# here I string split\n",
    "yelp['categories'] = yelp['categories'].str.split(';')\n",
    "# here I explode it out - making the df longer\n",
    "yelp = yelp.copy().explode('categories')\n",
    "# now I aggregate\n",
    "yelp['totals'] = yelp.groupby('categories')['review_count'].transform('sum')\n",
    "yelp[['categories','totals']].drop_duplicates().sort_values('totals', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf469ea7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Top Cool Votes - Yelp\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10060\n",
    "\n",
    "Find the review_text that received the highest number of  cool votes.\n",
    "Output the business name along with the review text with the highest number of cool votes.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "yelp_reviews['Rank']=yelp_reviews.cool.rank(method = 'dense', ascending = False)\n",
    "yelp_reviews[yelp_reviews.Rank ==1][['business_name','review_text']]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5776e217",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Income By Title and Gender - City of SF\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10077\n",
    "\n",
    "Find the average total compensation based on employee titles and gender. Total compensation is calculated by adding both the salary and bonus of each employee. However, not every employee receives a bonus so disregard employees without bonuses in your calculation. Employee can receive more than one bonus.\n",
    "Output the employee title, gender (i.e., sex), along with the average total compensation.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# get sals\n",
    "sf_emp = sf_employee[['id','employee_title','sex','salary']]\n",
    "# this fills 0 for people with null bonus \n",
    "sf_bonus['bonus']= sf_bonus['bonus'].fillna(0)\n",
    "# aggregate the tot bonus\n",
    "sf_bonus['tot_bonus'] = sf_bonus.groupby('worker_ref_id')['bonus'].transform('sum')\n",
    "#remove dupes\n",
    "sf_bonus = sf_bonus[['worker_ref_id','tot_bonus']].drop_duplicates()\n",
    "#merge the sets\n",
    "sf_gender = sf_emp.merge(sf_bonus, left_on ='id', right_on ='worker_ref_id', how = 'left').dropna().copy()[['sex','employee_title','salary','tot_bonus']]\n",
    "#calc averages \n",
    "sf_gender[['salary','tot_bonus']] = sf_gender.groupby(['employee_title','sex'])['salary','tot_bonus'].transform('mean')\n",
    "# drop dupes\n",
    "sf_gender = sf_gender.copy().drop_duplicates()\n",
    "# create the avg variable - remmeber the avg of the avg IS the avg\n",
    "sf_gender['avg_tot_comp']= sf_gender['salary']+sf_gender['tot_bonus']\n",
    "#filter\n",
    "sf_gender[['employee_title','sex','avg_tot_comp']].sort_values('employee_title')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b6481e8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Matching Similar Hosts and Guests - Airbnb\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10078\n",
    "\n",
    "Find matching hosts and guests pairs in a way that they are both of the same gender and nationality. Output the host id and the guest id of matched pair.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "matches = airbnb_hosts.merge(airbnb_guests, how = 'inner', on = (['nationality','gender']))[['host_id','guest_id']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c19e5cd9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Find the percentage of shippable orders - Google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10090\n",
    "\n",
    "Find the percentage of shippable orders. Consider an order is shippable if the customer's address is known.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# merge the files\n",
    "cust_order = customers[['id','address']].merge(orders[['cust_id','id']], left_on ='id', right_on ='cust_id', how = 'inner')[['cust_id', 'id_y','address']].rename(columns={'id_y':'order_id'})\n",
    "# create the shippable field\n",
    "cust_order['shippable'] = cust_order['address'].notnull().astype(int)\n",
    "cust_order['tot_orders'] = cust_order['order_id'].agg('count')\n",
    "cust_order\n",
    "cust_order['per_shippable'] = cust_order['shippable'].agg('sum')/cust_order['tot_orders']\n",
    "cust_order ['shippable'] = math.floor(cust_order['per_shippable'].unique()*100)\n",
    "cust_order['shippable'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db05a9b2",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Spam Posts - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10134\n",
    "\n",
    "Calculate the percentage of spam posts in all viewed posts by day. A post is considered a spam if a string \"spam\" is inside keywords of the post. \n",
    "Note that the facebook_posts table stores all posts posted by users. The facebook_post_views table is an action table denoting if a user has viewed a post.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# create spam variable\n",
    "facebook_posts['is_spam'] = facebook_posts['post_keywords'].apply(lambda row : 1 if 'spam' in row.lower() else 0)\n",
    "# merge the datasets\n",
    "fb_posts = facebook_posts[['post_date', 'post_id', 'is_spam']].merge(facebook_post_views, on = 'post_id')\n",
    "# we know they were seen - filter to just what we need\n",
    "fb_posts = fb_posts.copy()[(fb_posts['viewer_id']>0)][['post_date','is_spam','post_id']].drop_duplicates()\n",
    "# bam and solve\n",
    "fb_posts['spam_per'] = fb_posts.groupby('post_date')['is_spam'].transform('mean')*100\n",
    "fb_posts[['post_date','spam_per']].drop_duplicates().sort_values('post_date')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dd2580f3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Apple Product Counts - Apple\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10141\n",
    "\n",
    "\n",
    "We’re analyzing user data to understand how popular Apple devices are among users who have performed at least one event on the platform. Specifically, we want to measure this popularity across different languages. Count the number of distinct users using Apple devices —limited to \"macbook pro\", \"iphone 5s\", and \"ipad air\" — and compare it to the total number of users per language.\n",
    "Present the results with the language, the number of Apple users, and the total number of users for each language. Finally, sort the results so that languages with the highest total user count appear first.\n",
    "\n",
    "## my answer\n",
    "import pandas as pd\n",
    "# create a usable df of the user info with the \n",
    "pb_users = playbook_users[['user_id','language',]].drop_duplicates()\n",
    "pb_events = playbook_events[['user_id','device']].groupby('user_id')['device'].unique().apply(', '.join).reset_index()\n",
    "pb_tot = pb_users.merge(pb_events, how='inner', on = 'user_id')\n",
    "# create the apple user flag\n",
    "apple = ['macbook pro','iphone 5s','ipad air']\n",
    "# create the necessary variables\n",
    "pb_tot['apple'] = pb_tot['device'].apply(lambda x: 1 if any(device in x for device in apple) else 0)\n",
    "pb_tot['counts'] = 1\n",
    "pb_tot.groupby('language')[['apple','counts']].agg('sum').reset_index().sort_values('counts', ascending = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef5e3600",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "No Order Customers - Instacart\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10142\n",
    "\n",
    "Identify customers who did not place an order between 2019-02-01 and 2019-03-01.\n",
    "\n",
    "Include:\n",
    "•    Customers who placed orders only outside this date range.\n",
    "•    Customers who never placed any orders.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "# create order df\n",
    "orders['pur_range'] = orders['order_date'].apply(lambda x: 1 if x >= dt.datetime.strptime('2019-02-01', '%Y-%m-%d').date() and x<= dt.datetime.strptime('2019-03-01', '%Y-%m-%d').date() else 0)\n",
    "orders = orders.copy()[['id','cust_id','order_date','pur_range']]\n",
    "# create cut info df\n",
    "cust = customers.copy()[['id','first_name']]\n",
    "# merged df\n",
    "custs = cust.merge(orders, how = 'left', left_on = 'id',right_on = 'cust_id')[['id_x', 'first_name','order_date','pur_range']].rename(columns = {'id_x':'id',})\n",
    "#did any order happen in the range?\n",
    "custs['new_range'] = custs['pur_range'].transform(lambda a : 1 if a == 1 else 0)\n",
    "# max of that over customer level\n",
    "custs['ordered_inrange'] = custs.groupby('id')['new_range'].transform('max')\n",
    "custs[custs['ordered_inrange']==0][['first_name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cec16ac",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Number Of Units Per Nationality - Airbnb\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10156\n",
    "\n",
    "We have data on rental properties and their owners. Write a query that figures out how many different apartments (use unit_id) are owned by people under 30, broken down by their nationality. \n",
    "We want to see which nationality owns the most apartments, so make sure to sort the results accordingly.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# this gets the hosts and nationality info\n",
    "abb = airbnb_hosts.merge(airbnb_units, on = 'host_id')\n",
    "# filter by apartment and age\n",
    "abb = abb[(abb['unit_type'].str.lower()=='apartment')&(abb['age']<30)]\n",
    "# boom answer\n",
    "abb.groupby(['nationality'])['unit_id'].agg('nunique').reset_index()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0fc2db21",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Ranking Most Active Guests - Airbnb\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10159\n",
    "\n",
    "Identify the most engaged guests by ranking them according to their overall messaging activity. The most active guest, meaning the one who has exchanged the most messages with hosts, should have the highest rank. If two or more guests have the same number of messages, they should have the same rank. Importantly, the ranking shouldn't skip any numbers, even if many guests share the same rank. \n",
    "Present your results in a clear format, showing the rank, guest identifier, and total number of messages for each guest, ordered from the most to least active.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# rank over the full sum of messages, descending dense rank method\n",
    "airbnb_contacts['rank'] = airbnb_contacts.groupby('id_guest')['n_messages'].transform('sum').rank(method = 'dense', ascending = False)\n",
    "# get the full number of message by user\n",
    "airbnb_contacts['tot_mess'] = airbnb_contacts.groupby('id_guest')['n_messages'].transform('sum')\n",
    "# create the dataframe you want\n",
    "abnb = airbnb_contacts.copy()[['rank','id_guest','tot_mess']].drop_duplicates().sort_values('rank')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e8b8c33",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Number of Streets Per Zip Code - city of SF\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10182\n",
    "\n",
    "Count the number of unique street names for each postal code in the business dataset. Use only the first word of the street name, case insensitive (e.g., \"FOLSOM\" and \"Folsom\" are the same). \n",
    "If the structure is reversed (e.g., \"Pier 39\" and \"39 Pier\"), count them as the same street. Output the results with postal codes, ordered by the number of streets (descending) and postal code (ascending).\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "# get the cols I want to use\n",
    "sf = sf_restaurant_health_violations[['business_address','business_postal_code']].rename(columns = {'business_address':'address','business_postal_code':'postal_code'})\n",
    "# remove null zipcodes\n",
    "sf = sf.copy()[sf['postal_code'].notnull()]\n",
    "#here I test whether the first or 2 field is numeric and then keep only the non numeric part of the first or 2d field in split string\n",
    "sf['street']=sf['address'].transform(lambda x:  x.split(' ')[1].lower() if x.split(' ')[0].lower().isdigit() else x.split(' ')[1].lower().isdigit())\n",
    "#remove dupicate zipcode/street\n",
    "sf2 = sf.copy()[['postal_code','street']].drop_duplicates()\n",
    "# do the counting and get it right!\n",
    "sf2.groupby('postal_code')['street'].apply('count').reset_index().sort_values('street',ascending = False)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eaf2c37f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Meta/Facebook Accounts - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10296\n",
    "\n",
    "Calculate the ratio of accounts closed on January 10th, 2020 using the fb_account_status table.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# trim down to date we want\n",
    "fb_date = fb_account_status[fb_account_status['status_date']=='2020-01-10']\n",
    "# get total user count\n",
    "fb_date['count'] = 1\n",
    "# var for canceled account\n",
    "fb_date['canceled'] = fb_date['status'].transform(lambda x: 1 if x == 'closed' else 0)\n",
    "fb_date['date_canc_per'] = fb_date['canceled'].sum()/fb_date['count'].sum()\n",
    "fb_date['date_canc_per'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8829cab5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Share of Active Users - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 2005\n",
    "\n",
    "Calculate the percentage of users who are both from the US and have an 'open' status, as indicated in the fb_active_users table.\n",
    "\n",
    "## my answer \n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "# Vectorized calculation for 'us_open'\n",
    "fb_active_users['us_open'] = (\n",
    "    (fb_active_users['country'].str.lower() == 'usa') &\n",
    "    (fb_active_users['status'].str.lower() == 'open')\n",
    ").astype(bool)\n",
    "\n",
    "# Calculate percentage of US open accounts\n",
    "fb_active_users['us_active'] = (fb_active_users['us_open'].sum() / len(fb_active_users)) * 100\n",
    "fb_active_users['us_active'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b51e8a0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Finding Purchases - Amazon\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 10553\n",
    "\n",
    "Identify returning active users by finding users who made a second purchase within 7 days of any previous purchase. Output a list of these user_ids.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# what I need\n",
    "amazon = amazon_transactions[['user_id','created_at','id']]\n",
    "# cross join the dfs - thisll allow me to get all orders - with varying order id even if multiple on same day\n",
    "amz = amazon.merge(amazon, how = 'cross')\n",
    "amz = amz.copy()[(amz['user_id_x']==amz['user_id_y'])&(amz['created_at_y']>=amz['created_at_x'])&(amz['id_x']!=amz['id_y'])][['user_id_x','created_at_x','created_at_y']].rename(columns={'user_id_x':'user_id', 'created_at_x':'purchase_date','created_at_y':'another_purchase'}).sort_values(['user_id','purchase_date','another_purchase'])\n",
    "# create the variable for purchase within seven days\n",
    "amz['seven_days'] =  (amz['another_purchase']-amz['purchase_date']).dt.days\n",
    "# did this person order inside 7 days of another order?\n",
    "amz['within_7'] = amz.groupby('user_id')['seven_days'].transform(lambda x: 1 if x.min()<=7 else 0)\n",
    "# drop dupes!\n",
    "amz = amz.copy().drop_duplicates()\n",
    "# boom answer\n",
    "amz[amz['within_7']==1]['user_id'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "185c48c7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Premium Accounts - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 2097\n",
    "\n",
    "You have a dataset that records daily active users for each premium account. A premium account appears in the data every day as long as it remains premium. However, some premium accounts may be temporarily discounted, meaning they are not actively paying—this is indicated by a final_price of 0.\n",
    "For each of the first 7 available dates in the dataset, count the number of premium accounts that were actively paying on that day. Then, track how many of those same accounts are still premium and actively paying exactly 7 days later, based solely on their status on that 7th day (i.e., both dates must exist in the dataset). Accounts are only counted if they appear in the data on both dates.\n",
    "\n",
    "Output three columns:\n",
    "•   The date of initial calculation.\n",
    "•   The number of premium accounts that were actively paying on that day.\n",
    "•   The number of those accounts that remain premium and are still paying after 7 days.\n",
    "\n",
    "## my answer\n",
    "import pandas as pd\n",
    "import datetime as td\n",
    "# get unique users\n",
    "users = pd.DataFrame(premium_accounts_by_day['account_id'].unique()).rename(columns={0:'acct_id'})\n",
    "# get unique days\n",
    "days = pd.DataFrame(premium_accounts_by_day['entry_date'].unique()).rename(columns={0:'first_day'})\n",
    "# get 7 days from the first date\n",
    "days['last_day'] = days['first_day'].transform(lambda x : x + td.timedelta(days=7))\n",
    "# limit to first 7 days of the project\n",
    "days = days[days['first_day']<='2022-02-13']\n",
    "# get every day for every user\n",
    "user_days = users.merge(days, how = 'cross').sort_values(['acct_id','first_day'])\n",
    "# get whether they were premium on first days\n",
    "user_days1 = user_days.merge(\n",
    "premium_accounts_by_day[['account_id','entry_date','final_price']],\n",
    "left_on = ['acct_id', 'first_day'],\n",
    "right_on = ['account_id','entry_date'], \n",
    "how = 'left')[['acct_id','first_day','last_day','final_price']].rename(columns={'final_price':'prem1'})\n",
    "#get premium on day 8\n",
    "user_days2 = user_days1.merge(\n",
    "premium_accounts_by_day[['account_id','entry_date','final_price']],\n",
    "left_on = ['acct_id', 'last_day'],\n",
    "right_on = ['account_id','entry_date'], \n",
    "how = 'left')[['first_day','prem1','final_price']].rename(columns={'final_price':'prem7'})\n",
    "# fill na with 0\n",
    "user_days2 = user_days2.copy().fillna(0)\n",
    "# make the prem1, prem7 1 if > 0\n",
    "user_days2[['prem1', 'prem7']] = user_days2[['prem1', 'prem7']].applymap(lambda x: 1 if x > 0 else x)\n",
    "# change the prem7 to get right counts!\n",
    "user_days2['prem7'] = user_days2.apply(lambda row: 0 if row['prem1']==0 else row['prem7'],axis=1)\n",
    "#this gets the final answer!\n",
    "user_days2.groupby('first_day')[['prem1','prem7']].agg('sum').reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af0bd969",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Election Results - Deloitte\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 2099\n",
    "\n",
    "The election is conducted in a city and everyone can vote for one or more candidates, or choose not to vote at all. Each person has 1 vote so if they vote for multiple candidates, their vote gets equally split across these candidates. For example, if a person votes for 2 candidates, these candidates receive an equivalent of 0.5 vote each. Some voters have chosen not to vote, which explains the blank entries in the dataset.\n",
    "\n",
    "Find out who got the most votes and won the election. Output the name of the candidate or multiple names in case of a tie.\n",
    "To avoid issues with a floating-point error you can round the number of votes received by a candidate to 3 decimal places.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "# get counts of votes by each voter, get the part of each vote\n",
    "voting_results['vote_sum'] = 1/(voting_results.groupby('voter')['candidate'].transform('count'))\n",
    "# drop missing values, get only candidates\n",
    "vote_results1 = voting_results.copy()[['candidate','vote_sum']].dropna()\n",
    "# sum_everything\n",
    "votes2 = vote_results1.groupby('candidate')['vote_sum'].sum().reset_index()\n",
    "# get rank\n",
    "votes2['rank'] = votes2['vote_sum'].rank(method = 'dense', ascending=False)\n",
    "# filter and done\n",
    "votes2[votes2['rank']==1]['candidate']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0126a77",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Flags per Video - Netflix\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 2102\n",
    "\n",
    "For each video, find how many unique users flagged it. A unique user can be identified using the combination of their first name and last name. Do not consider rows in which there is no flag ID.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# eliminate blank flag_id, then get everything we need to deduplicate at right level - copy!\n",
    "user_flags = user_flags[(~user_flags['flag_id'].isnull())][['user_firstname','user_lastname','video_id']].drop_duplicates().copy()\n",
    "# now that this is unique - we only need the video_id - dont forget to reset index\n",
    "flags = user_flags['video_id'].reset_index()\n",
    "flags.groupby('video_id').count().reset_index().rename(columns = {'index':'counts'})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ff5075a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "User with Most Approved Flags - google\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 2104\n",
    "\n",
    "Which user flagged the most distinct videos that ended up approved by YouTube? Output, in one column, their full name or names in case of a tie. In the user's full name, include a space between the first and the last name.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# merge the sets\n",
    "flag_info = user_flags[['user_firstname','user_lastname','video_id','flag_id']].merge(flag_review[[\n",
    "'flag_id','reviewed_outcome']], how = 'left', left_on = 'flag_id',right_on='flag_id').drop_duplicates()\n",
    "# create the new variable\n",
    "flag_info['name'] = flag_info['user_firstname'] + ' ' + flag_info['user_lastname']\n",
    "# clean up the \n",
    "flag_info = flag_info[(~flag_info['name'].isnull())&(flag_info['reviewed_outcome'].str.lower()=='approved')]\n",
    "# get only necessary info\n",
    "flag_info = flag_info.copy()[['name','video_id']].drop_duplicates()\n",
    "# get counts\n",
    "flag_info['counts'] = flag_info.groupby('name')['video_id'].transform('nunique')\n",
    "# rank\n",
    "flag_info['rank'] = flag_info.copy()['counts'].rank(ascending=False, method='dense')\n",
    "# drop dupes\n",
    "flag_info = flag_info[flag_info['rank']==1]['name'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56267261",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Find Students At Median Writing - General Assembly\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9610\n",
    "\n",
    "Identify the IDs of students who scored exactly at the median for the SAT writing section.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# this literally just matches the students who get the median sat writing score\n",
    "sat_scores[sat_scores['sat_writing']==sat_scores['sat_writing'].agg('median')]['student_id'] "
   ]
  },
  {
   "cell_type": "raw",
   "id": "0de9de90",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Top 10 Songs 2010 - Spotify\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9650\n",
    "\n",
    "Find the top 10 ranked songs in 2010. Output the rank, group name, and song name, but do not show the same song twice. Sort the result based on the rank in ascending order.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Start writing code\n",
    "billboard_top_100_year_end[(billboard_top_100_year_end['year_rank']<=10)&(billboard_top_100_year_end['year']==2010)][['year_rank','group_name','song_name']].sort_values('year_rank').drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a17fb0d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Processed Ticket Rate By Type - Meta\n",
    "\n",
    "Last Updated: June 2025\n",
    "\n",
    "Medium ID 9781\n",
    "\n",
    "Find the processed rate of tickets for each type. The processed rate is defined as the number of processed tickets divided by the total number of tickets for that type. Round this result to two decimal places.\n",
    "\n",
    "## my answer\n",
    "# Import your libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# create a flag thata flag is opened\n",
    "facebook_complaints['opened'] = np.where(facebook_complaints['processed']==facebook_complaints['processed'],1,0)\n",
    "# note when it is closed\n",
    "facebook_complaints['closed'] = np.where(facebook_complaints['processed']==1,1,0)\n",
    "# create the new dataset\n",
    "fb_comp = facebook_complaints.groupby('type')[['opened','closed']].agg('sum').reset_index()\n",
    "#lastly make the variable\n",
    "fb_comp['percent'] = fb_comp['closed']/fb_comp['opened']\n",
    "#boom\n",
    "fb_comp[['type','percent']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
